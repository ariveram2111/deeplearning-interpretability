# Interpretability in Deep Learning models - Applying some examples to VGG16 model - Computer Vision

This repository contains a python notebook that implements 4 interpretability methods and apply them as examples to VGG16 model. These are the following:
- Gradient ascent in the input space using the *channel function* as objective function to visualize filters learned by the neural network with synthetic images.
- Gradient ascent in the input space using the *class function* as objective function to visualize output classes with synthetic images.
- GradCAM method to visualize the most important features that a specific layer is paying more attention to classify a specific sample.
- GradCAM method to visualize what are the most important features that the neural network is using to classify a specific sample, using all convolutional layers.

## Related article

You can find a more detailed explanation of this notebook in [this article from ***vic.ai***](https://www.vic.ai/blog/the-xai-problem-in-machine-learning)

## Author and contact

**Alberto Rivera Mart√≠nez**. To contact me, use my github contact details. You can also contact me through the following platforms:
- [Twitter](https://twitter.com/ariveram2111)
- [LinkedIn](https://es.linkedin.com/in/alberto-rivera-mart%C3%ADnez-351558177)

